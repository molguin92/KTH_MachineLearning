\documentclass{kthreport}
% default language is English, but you can use Swedish definitions like this:
% \documentclass[swedish]{kthreport}

% Remember that in order for the class to find the KTH logo, you need to
% have it in your path, for example:
% export TEXINPUTS=/path/to/logo/location//:$TEXINPUTS

\usepackage{amsmath}
\usepackage[libertine,cmintegrals,cmbraces,vvarbb]{newtxmath}
\usepackage[backend=biber, sorting=none]{biblatex}
\addbibresource{bibl.bib}
\usepackage{listings}
\usepackage{xcolor}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{lightgray}{rgb}{0.95, 0.95, 0.95}
\definecolor{mauve}{rgb}{0.58,0,0.82}
\definecolor{mygreen}{rgb}{0,0.6,0}
\definecolor{mygray}{rgb}{0.5,0.5,0.5}
\definecolor{mymauve}{rgb}{0.58,0,0.82}
\lstdefinestyle{MyPython}{
	language=Python,
	frame=Lbtr,
	xleftmargin=\parindent,
	captionpos=b,
	aboveskip=3mm,
	belowskip=3mm,
	showstringspaces=false,
	columns=flexible,
	basicstyle={\small\ttfamily},
	numbers=left,
	numberstyle=\tiny\color{gray},
	keywordstyle=\color{purple},
	commentstyle=\color{gray},
	stringstyle=\color{dkgreen},
	breaklines=true,
	breakatwhitespace=true,
	tabsize=4,
	morekeywords={traci, print},
	otherkeywords={},
	backgroundcolor=\color{lightgray},
	escapeinside={/l*}{*l/}
}

%\DeclareFontFamily{\encodingdefault}{\ttdefault}{\hyphenchar\font=`\-}

\title{Final Assignment}
\subtitle{DD3431 Machine Learning (PhD variant)}
\author{Manuel Osvaldo Olgu√≠n}
%\diarienr{99999-99}

\begin{document}
\maketitle

This final report for the DD3431 Machine Learning course describes the application of Linear Support Vector Machines to a multi-class, multi-output classification problem modeling the resource allocation strategies for basestations in a 5G environment. The classifier was adapted to the multi-class nature of the output.

\section{Theoretical description of the problem}

\subsection{Description of the data}\label{sec:data}

The data used for this study corresponds to samples of the resource allocation for a communication system composed of exactly three basestations and up to a maximum of 20 user devices. The area of interest studied within this communication scenario is modeled as a map; more specifically, it represents an area of 2400 m$^{2}$ divided into a grid of 2m$\times$2m cells. This grid is encoded as a matrix, with values between 0 and 2 indicating the occupancy status of each cell:
\begin{itemize}
	\item 0 for empty cells,
	\item 1 for cells occupied by a user device,
	\item 2 for cells occupied by a basestation.
\end{itemize}

The occupancy data for each user could be either perfect or have an inaccuracy defined by a normal distribution with three possible standard deviations: 0.1, 0.25 or 0.4. Thus, the model had to be trained a total of four times, one for each variation of the positioning inaccuracy (we'll call them ``scenarios''). 

The input data for the learning algorithm corresponds then to the vectorized form of this matrix, i.e. a vector of 600\footnote{$\frac{2400}{(2\times2)} = 600$} elements, each with a value $v_i \in \{0, 1, 2\}$.

On the other hand, the output data for the learning algorithm corresponds to encoded variables representing the basestation--user device association and the resource allocation information for each sample. The exact details of this encoding and the information carried within escape the scope of this report, but it is of importance to note that:
\begin{itemize}
	\item there are 9 of these output variables per input sample (3 for each basestation in the model);
	\item these output variables are mutually independent;
	\item they have discrete integer values that range between 0 and 12 288;
	\item these values can be repeated across variables;
	\item finally, the order in which these values appear in the output is relevant (i.e. permutations of the same values correspond to different output classes!).
\end{itemize} 

In summary, the structure of an arbitrary sample looks like the following:

\[\underbrace{0, 1, 0, 0, 2, 0, \ldots 0, 0, 0, 0, 0, 1,}_{\text{600 input features}}\underbrace{4567, 23, \ldots , 1337}_{\text{9 output labels}}\]

In total, 72 000 samples in this format for each scenario were provided for the training of the model, with two thirds (48 000) of these used for fitting and the rest (24 000) used for validation purposes. The complete dataset was also used for 10-fold cross-validation, again for each scenario.

\subsection{Adapting the data} \label{sec:problemtransform}

As described in the previous section, the data represents a multi-class, multi-label problem with a high dimensionality both in terms of input features and classes, with the additional restriction that the order of the output labels matters. \autocite{tsoumakas2009mining}

\subsection{Choice and adaptation of classifier}\label{sec:cfchoice}

The dataset in question is part of ongoing research at the Department of Information Science and Engineering of the School of Electrical Engineering, and has already been modeled with great success using Random Forests and Neural Networks. The application of Support Vector Machines was then a natural step given the models' popularity in networking research literature; the specific application of the Linear Kernel for SVMs was a result of experimentation with the dataset, where initial experiments exposed the linearly separatable nature of the output variables.

Support Vector Machines are binary classifiers though, and thus additional modifications are required to adapt these classifiers to multiclass problems as the one in question. Specifically, the following techniques for adapting binary classifiers to multiclass applications were identified from literature \autocite{hsu2002comparison,tax2002multiclass}:

\begin{itemize}
	\item One-vs-One Method: For $n$ classes, this method constructs $n(n-1)/2$ classifiers, each comparing a pair of classes from the training set. For prediction, all $n(n-1)/2$ classifiers are applied to an unseen sample, whose final class corresponds to the class with the most ``votes'' after processing. In case of a tie, it selects the class with the highest total confidence, obtained by aggregating the confidence scores of each binary classifier.
	\item One-vs-Rest Method: Constructs $n$ classifiers, each comparing one class in the training set with the rest (i.e. each classifier determines if a sample belongs to a specific class or not). At prediction time, these $n$ classifiers are applied to the unseen sample and it is once again classified according to the majority vote.
\end{itemize}

Other multiclass adaptations of binary classifiers, like DAGSVM \autocite{chen2009dagsvm} and DDAG \autocite{platt2000ddag} were considered as well, but were ultimately considered overly complex for the problem at hand and dismissed.



\section{Implementation}

\subsection{Language and libraries used}

The language chosen for this project was Python 3.6, given its extensive support for scientific programming, data analysis and machine learning in the form of libraries. In particular, the libraries \textbf{scikit-learn} (and its multilabel extension, \textbf{scikit-multilearn}), \textbf{scipy}, \textbf{matplotlib} and \textbf{numpy} were used for the implementation \autocite{scikit-learn,scikit-multilearn,scipy,matplotlib,numpy}.

\subsection{Choice of classifier}

Based on the analysis detailed in section \ref{sec:cfchoice}, \textbf{sklearn.svm.LinearSVC} was selected as the base classifier to be used for this problem, as it corresponds to an implementation of a multi-class Support Vector Machine using the \emph{One-vs-Rest} method and a linear kernel. Tests were also conducted with \textbf{sklearn.svm.SVC}, which implements a multi-class SVM using the \emph{One-vs-One} method and a RBF-kernel, but its runtime proved to be cumbersomely large\footnote{Not a fair comparison, but for the unoptimized SVC case the runtime was over 3 hours for fitting and predicting, whereas the final optimized runtime for the LinearSVC is, on average, 110 seconds.}.

The classifier was then extended to work on multilabel outputs through the \textbf{skmultilearn.problem\_transform.LabelPowerSet} class, which implements the \emph{Label Powerset} problem transformation as detailed in \ref{sec:problemtransform}, thus treating every distinct label combination in the training data as a separate class to pass to the multiclass classifier.

\subsection{Parsing and adapting the data}

The data provided for building the model consisted of 72 000 samples for each of the four positional accuracy values detailed in the problem description (section \ref{sec:data}), divided into two files each: two thirds (48 000) of the samples for training, and one third (24 000) for validation. The format of the input files was one sample per line: 600 integers with values in $\{0, 1, 2\}$ representing the input features, followed by 9 integers with values in $[0, 12288]$ representing the output variables (everything separated by commas, see example below).

\begin{lstlisting}[captionpos=b,
				basicstyle={\small\ttfamily},
				numbers=left, 
				numberstyle=\tiny\color{gray},
				caption={Abbreviated example of input samples.}]
0,0,0,...,0,2,0,...,3968,3841,528,8080,4209,6273,9201,9073,8592
0,0,0,...,0,2,0,...,80,2545,2417,6145,8176,4112,8443,11761,8736
0,0,0,...,0,2,0,...,3680,0,0,4112,8017,7408,10241,8272,11505
\end{lstlisting}

This data was read parsed \textbf{numpy}'s \textbf{loadfromtxt()} function, and then split into separate arrays for the input and output data (labels). The input data was passed ``as-is'' to the classifier, but the output data required additional processing  

\printbibliography

\end{document}
